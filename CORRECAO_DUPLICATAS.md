# Corre√ß√£o de Duplicatas - Job ID

## Problemas Identificados

### 1. **Busca de Job IDs sem Pagina√ß√£o**
**Problema:** A fun√ß√£o `salvar_dados_no_supabase()` buscava job_ids existentes sem pagina√ß√£o:
```python
result_existentes = supabase.table('fox_deliveries').select('job_id').execute()
```

**Impacto:** O Supabase/PostgREST tem limite de 1000 registros por query. Se o banco tiver mais de 1000 entregas, apenas os primeiros 1000 job_ids eram verificados, permitindo duplicatas para registros al√©m desse limite.

**Solu√ß√£o:** Implementada busca com pagina√ß√£o para obter TODOS os job_ids:
```python
while True:
    result_existentes = supabase.table('fox_deliveries').select('job_id').range(offset, offset + page_size - 1).execute()
    if not result_existentes.data or len(result_existentes.data) < page_size:
        break
    offset += page_size
```

### 2. **Cache Atualizado com Duplicatas**
**Problema:** O cache era atualizado com TODOS os registros da planilha, incluindo duplicatas:
```python
cache_atualizado = atualizar_cache_metricas_incremental(df)  # df completo
```

**Impacto:** M√©tricas ficavam infladas, contando duplicatas que n√£o foram inseridas no banco.

**Solu√ß√£o:** 
- Fun√ß√£o `salvar_dados_no_supabase()` agora retorna lista de job_ids efetivamente inseridos
- Cache √© atualizado APENAS com registros novos:
```python
job_ids_inseridos = set(resultado_db.get('job_ids_inseridos', []))
df_novos = df[df['Job ID'].isin(job_ids_inseridos)]
cache_atualizado = atualizar_cache_metricas_incremental(df_novos)
```

## Mudan√ßas Implementadas

### `salvar_dados_no_supabase()`
1. ‚úÖ Busca paginada de TODOS os job_ids existentes
2. ‚úÖ Retorna lista de job_ids efetivamente inseridos
3. ‚úÖ Logs melhorados com contador de duplicatas

### Upload Endpoint
1. ‚úÖ Filtra DataFrame para incluir APENAS registros novos
2. ‚úÖ Cache atualizado apenas com dados realmente inseridos
3. ‚úÖ Tratamento para caso de 100% duplicatas

## Como Testar

### Teste 1: Upload da mesma planilha 2x
```bash
# 1. Fazer upload da planilha
curl -X POST http://127.0.0.1:5000/upload -F "file=@planilha.xlsx"

# Resposta esperada:
# {
#   "sucesso": true,
#   "registros_inseridos": 567,
#   "duplicatas_evitadas": 0
# }

# 2. Fazer upload DA MESMA planilha novamente
curl -X POST http://127.0.0.1:5000/upload -F "file=@planilha.xlsx"

# Resposta esperada:
# {
#   "sucesso": true,
#   "registros_inseridos": 0,
#   "duplicatas_evitadas": 567,
#   "mensagem": "...e 567 duplicatas evitadas"
# }
```

### Teste 2: Verificar contagem no banco
```bash
# Verificar m√©tricas
curl http://127.0.0.1:5000/metricas-resumo-banco | python3 -c "import sys, json; data = json.load(sys.stdin); print('Total deliveries:', data['metrics']['total_deliveries']['value'])"

# Deve retornar o mesmo n√∫mero ap√≥s 2 uploads da mesma planilha
```

### Teste 3: Upload com banco > 1000 registros
```bash
# Com mais de 1000 registros no banco, fazer upload de duplicatas
# Verificar nos logs do backend:
# "Total de X Job IDs √∫nicos no banco"
# "Total de Y duplicatas evitadas"
```

## Logs de Exemplo

### Upload com Novos Dados
```
Verificando Job IDs existentes para evitar duplicatas...
Total de 2743 Job IDs √∫nicos no banco
Lote 1 inserido: 100 registros
Lote 2 inserido: 100 registros
...
‚úÖ Total de 567 registros NOVOS salvos no Supabase
üîÑ Iniciando atualiza√ß√£o do cache de m√©tricas...
   Registros NOVOS: 567
   DataFrame filtrado: 567 linhas (apenas novos registros)
‚úÖ Cache de m√©tricas atualizado com SUCESSO!
```

### Upload com 100% Duplicatas
```
Verificando Job IDs existentes para evitar duplicatas...
Total de 3310 Job IDs √∫nicos no banco
‚ö†Ô∏è Total de 567 duplicatas evitadas
Nenhum dado novo para inserir (todos eram duplicatas ou sem Job ID)
‚ö†Ô∏è Nenhum registro novo para atualizar cache (todos duplicatas)
```

## Resultado Final

‚úÖ **Duplicatas Evitadas:** Job IDs duplicados n√£o s√£o inseridos no banco
‚úÖ **Cache Correto:** M√©tricas calculadas APENAS com registros novos
‚úÖ **Escalabilidade:** Funciona com qualquer quantidade de registros no banco
‚úÖ **Performance:** Busca otimizada com pagina√ß√£o em lotes de 1000
